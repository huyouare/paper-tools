[["\n1 Introduction", "The goal of LLaMA is to create open-source language models that perform well under various inference budgets by training on more tokens than existing models. LLaMA is trained only on publicly available data, and LLaMA-13B outperforms GPT-3 on most benchmarks, while LLaMA-65B is competitive with other large language models. The paper discusses the modifications made to the transformer architecture and training methods used to achieve these results. Additionally, the authors address biases and toxicity encoded in LLaMA models.\n\nKey takeaways:\n* LLaMA aims to create open-source language models that perform well under various inference budgets.\n* LLaMA models are trained only on publicly available data.\n* LLaMA-13B outperforms GPT-3 on most benchmarks and LLaMA-65B is competitive with other large language models.\n* Modifications were made to the transformer architecture and training methods to achieve improved performance.\n* Biases and toxicity encoded in LLaMA models are addressed by the authors."], ["\n2 Approach", "The authors present LLaMA, an open-source large language model (LLM) based on the transformer architecture trained on a dataset of 1.4T tokens from various sources. LLaMA introduces optimizations like pre-normalization, a SwiGLU activation function, and rotary embeddings, and uses an efficient implementation of the causal multi-head attention to reduce memory usage and runtime. It also overlaps the computation of activations and GPU communication to speed up training. Despite being newly introduced, LLaMA achieves competitive zero-shot performance on common sense reasoning tasks compared to other LLMs.\n\nKey takeaways:\n* LLaMA introduces optimizations in pre-training, such as pre-normalization, rotary embeddings and a SwiGLU activation function\n* The model makes use of an efficient implementation of causal multi-head attention to reduce memory usage and runtime\n* LLaMA achieves good zero-shot performance on common sense reasoning tasks"], ["\n3 Main results", "LLaMA is a newly introduced open and efficient foundation language model that has been tested on various zero-shot and few-shot tasks, including common sense reasoning."], ["\n4 Instruction Finetuning", "Finetuning **LLaMA** on instructions **data** improves performance on **MMLU** with a noticeable improvement even with a small amount of finetuning. The **LLaMA-I** model outperforms existing models on MMLU but falls behind state-of-the-art results. Performance for the 57 tasks is given in Table 16 of the appendix.\n\nKey takeaways:\n* Finetuning LLaMA on instruction data improves performance on MMLU.\n* LLaMA-I surpasses existing models but still lags behind the state-of-the-art results on MMLU.\n* Detailed performance on MMLU for 57 tasks is located in Table 16 of the appendix."], ["\n5 Bias, Toxicity and Misinformation", "The study evaluates the harm of LLaMA-65B regarding its potential to generate toxic content, stereotypes, and misinformation. It is shown that large language models reproduce and amplify the biases present in the training data. LLaMA-65B's toxicity increases with the size of the model, while biases related to gender and occupation appear in the WinoGender benchmark. In the CrowS-Pairs benchmark, the model is biased in the religion, age, and gender categories compared to GPT-3 and OPT-175B. Although LLaMA scores higher than GPT-3 in the TruthfulQA benchmark, its rate of correct answers is still low. However, the evaluations provided are insufficient in fully assessing the risks posed by LLaMA-65B or other large language models.\n\nKey takeaways:\n* Large language models amplify existing biases present in training data\n* LLaMA-65B's toxicity increases with model size, and it exhibits biases related to gender, occupation, religion, and age\n* LLaMA-65B is less accurate than GPT-3 in generating truthful answers but still has potential to generate misinformation"], ["\n6 Carbon footprint", "The LLaMA language models have a high carbon footprint due to massive energy consumption during training. The study estimates that training the models used 2,638 MWh and resulted in emissions of 1,015 tCO2eq. To reduce carbon emissions, researchers suggest using smaller pre-trained models for further research. The carbon footprint was calculated using a formula that estimated the Watt-hour required to train the models and the tons of carbon emissions.\n\nKey takeaways:\n* LLaMA language models have high carbon footprint\n* Training used 2,638 MWh, emitting 1,015 tCO2eq\n* Utilizing pre-trained smaller models could reduce future emissions"], ["\n7 Related work", "Language models, which have been used extensively in natural language processing, have a history of both model and dataset scaling. Recent advancements include BERT, GPT-2, T5, Megatron-LM, GPT-3, and Jurassic-1, which all show great promise. The relationship between the size of the model and dataset and the performance of the system has been characterized by power laws. It has been demonstrated that these relationships have an impact on the performance of deep learning models, and changes in the learning-rate schedule have improved transformer-based language models.\n\nKey takeaways:\n* Language models are essential in natural language processing and can measure machine intelligence.\n* Recent neural network advancements include BERT, GPT-2, T5, Megatron-LM, GPT-3, and Jurassic-1.\n* Performance is influenced by model and dataset size and can be improved by modifying the learning-rate schedule."], ["\n8 Conclusion", "The paper presents the LLaMA-13B and LLaMA-65B language models, which outperform other foundation models like GPT-3, Chinchilla-70B, and PaLM-540B. These models were trained solely using publicly available data, making them more accessible to the research community. Training these models on instructions shows promise in improving their performance, and the authors plan to release larger models in the future with more extensive pretraining corpora. The release of LLaMA models will speed up the development of large language models while improving their robustness and mitigating issues like toxicity and bias.\n\nKey takeaways:\n* LLaMA-13B and LLaMA-65B language models surpass other foundation models in performance.\n* The models were trained on publicly available data without using proprietary datasets.\n* The models' release to the research community will improve the development of large language models, enhance their robustness, and mitigate issues like toxicity and bias. \n* Fine-tuning these models on instructions offers promising results. \n* The authors plan to release larger models in the future trained on more extensive pretraining corpora to improve their performance."]]